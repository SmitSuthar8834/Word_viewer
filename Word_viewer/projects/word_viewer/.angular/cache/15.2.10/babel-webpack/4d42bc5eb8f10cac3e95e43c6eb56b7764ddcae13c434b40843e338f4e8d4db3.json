{"ast":null,"code":"var Token = require(\"./Token\");\nvar StringSource = require(\"./StringSource\");\nexports.RegexTokeniser = RegexTokeniser;\nfunction RegexTokeniser(rules) {\n  rules = rules.map(function (rule) {\n    return {\n      name: rule.name,\n      regex: new RegExp(rule.regex.source, \"g\")\n    };\n  });\n  function tokenise(input, description) {\n    var source = new StringSource(input, description);\n    var index = 0;\n    var tokens = [];\n    while (index < input.length) {\n      var result = readNextToken(input, index, source);\n      index = result.endIndex;\n      tokens.push(result.token);\n    }\n    tokens.push(endToken(input, source));\n    return tokens;\n  }\n  function readNextToken(string, startIndex, source) {\n    for (var i = 0; i < rules.length; i++) {\n      var regex = rules[i].regex;\n      regex.lastIndex = startIndex;\n      var result = regex.exec(string);\n      if (result) {\n        var endIndex = startIndex + result[0].length;\n        if (result.index === startIndex && endIndex > startIndex) {\n          var value = result[1];\n          var token = new Token(rules[i].name, value, source.range(startIndex, endIndex));\n          return {\n            token: token,\n            endIndex: endIndex\n          };\n        }\n      }\n    }\n    var endIndex = startIndex + 1;\n    var token = new Token(\"unrecognisedCharacter\", string.substring(startIndex, endIndex), source.range(startIndex, endIndex));\n    return {\n      token: token,\n      endIndex: endIndex\n    };\n  }\n  function endToken(input, source) {\n    return new Token(\"end\", null, source.range(input.length, input.length));\n  }\n  return {\n    tokenise: tokenise\n  };\n}","map":null,"metadata":{},"sourceType":"script","externalDependencies":[]}